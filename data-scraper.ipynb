{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify article urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    'Towards Data Science': 'https://towardsdatascience.com/archive/{0}/{1:02d}/{2:02d}'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = 'images'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leap(year):\n",
    "    if year % 4 != 0:\n",
    "        return False\n",
    "    elif year % 100 != 0:\n",
    "        return True\n",
    "    elif year % 400 != 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create WebScraper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    # initial values to specify\n",
    "    def __init__(self, selected_days, urls, year):\n",
    "        self.selected_days = selected_days\n",
    "        self.urls = urls\n",
    "        self.year = year\n",
    "        self.data = []\n",
    "        self.article_id = 0\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5), wait=wait_fixed(10)) # retry if request fails, wait for 10s, try 5 times\n",
    "    # make request to url to get response\n",
    "    def make_request(self, url):\n",
    "        response = requests.get(url, allow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "\n",
    "    # convert 'day and year' to 'month and day'\n",
    "    def convert_day(self, day, year):\n",
    "        month_days = [31, 29 if is_leap(year) else 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        m = 0\n",
    "        d = 0\n",
    "        while day > 0:\n",
    "            d = day\n",
    "            day -= month_days[m]\n",
    "            m += 1\n",
    "        return (m, d)\n",
    "\n",
    "    # get the image and save it to the destination folder\n",
    "    def get_img(self, img_url, dest_folder, dest_filename):\n",
    "        ext = img_url.split('.')[-1]\n",
    "        if len(ext) > 4:\n",
    "            ext = 'jpg'\n",
    "        dest_filename = f'{dest_filename}.{ext}'\n",
    "        with open(f'{dest_folder}/{dest_filename}', 'wb') as f:\n",
    "            f.write(requests.get(img_url, allow_redirects=False).content)\n",
    "        return dest_filename\n",
    "\n",
    "    # get the number of claps for the article\n",
    "    def get_claps(self, claps_str):\n",
    "        if (claps_str is None) or (claps_str == '') or (claps_str.split is None):\n",
    "            return 0\n",
    "        split = claps_str.split('K')\n",
    "        claps = float(split[0])\n",
    "        claps = int(claps*1000) if len(split) == 2 else int(claps)\n",
    "        return claps\n",
    "\n",
    "    # main function to do the web scraping, looping through each article on each day and saving to self.data\n",
    "    def scrape(self):\n",
    "        i = 0\n",
    "        n = len(self.selected_days)\n",
    "\n",
    "        for d in self.selected_days: # loop through days\n",
    "            i += 1\n",
    "            month, day = self.convert_day(d, self.year)\n",
    "            date = '{0}-{1:02d}-{2:02d}'.format(self.year, month, day)\n",
    "            print(f'{i} / {n} ; {date}')\n",
    "\n",
    "            for publication, url in self.urls.items():\n",
    "                try:\n",
    "                    response = self.make_request(url.format(self.year, month, day))\n",
    "                except requests.exceptions.HTTPError as errh:\n",
    "                    print(\"HTTP Error:\", errh)\n",
    "                    continue\n",
    "                except requests.exceptions.ConnectionError as errc:\n",
    "                    print(\"Connection Error:\", errc)\n",
    "                    continue\n",
    "                except requests.exceptions.Timeout as errt:\n",
    "                    print(\"Timeout Error:\", errt)\n",
    "                    continue\n",
    "                except requests.exceptions.RequestException as err:\n",
    "                    print(\"Request Exception:\", err)\n",
    "                    continue\n",
    "\n",
    "                page = response.content\n",
    "                soup = BeautifulSoup(page, 'html.parser')\n",
    "                articles = soup.find_all(\"div\", class_=\"postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls\")\n",
    "\n",
    "                for article in articles: # loop through articles on that day\n",
    "                    title = article.find(\"h3\", class_=\"graf--title\")\n",
    "                    if title is None:\n",
    "                        continue\n",
    "                    title = title.contents[0]\n",
    "                    self.article_id += 1\n",
    "                    subtitle = article.find(\"h4\", class_=\"graf--subtitle\")\n",
    "                    subtitle = subtitle.contents[0] if subtitle is not None else ''\n",
    "                    image = article.find(\"img\", class_=\"graf-image\")\n",
    "                    image = '' if image is None else self.get_img(image['src'], 'images', f'{self.article_id}')\n",
    "                    article_url = article.find_all(\"a\")[3]['href'].split('?')[0]\n",
    "                    if len(article.find_all(\"button\")) == 0:\n",
    "                        claps = self.get_claps('99999')\n",
    "                    else:\n",
    "                        claps = self.get_claps(article.find_all(\"button\")[1].contents[0])\n",
    "                    reading_time = article.find(\"span\", class_=\"readingTime\")\n",
    "                    reading_time = 0 if reading_time is None else int(reading_time['title'].split(' ')[0])\n",
    "                    responses = article.find_all(\"a\")\n",
    "                    if len(responses) == 7:\n",
    "                        responses = responses[6].contents[0].split(' ')\n",
    "                        if len(responses) == 0:\n",
    "                            responses = 0\n",
    "                        else:\n",
    "                            responses = responses[0]\n",
    "\n",
    "                    # append data to self.data\n",
    "                    self.data.append([self.article_id, article_url, title, subtitle, image, claps, responses, reading_time, publication, date])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022 # specify year\n",
    "selected_days = [i for i in range(1, 367 if is_leap(year) else 366)] # specify number of days (e.g. first and second day of the year would => [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 1 ; 2022-01-01\n"
     ]
    }
   ],
   "source": [
    "WebScraperClass = WebScraper(selected_days, urls, year) # create WebScraper class\n",
    "\n",
    "WebScraperClass.scrape() # scrape articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = WebScraperClass.data # save data into a 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep columns needed\n",
    "medium_df = pd.DataFrame(data, columns=[\n",
    "    'id', 'url', 'title', 'subtitle',\n",
    "    'image', 'claps', 'responses',\n",
    "    'reading_time', 'publication', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of data\n",
    "medium_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>image</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://towardsdatascience.com/5-ways-to-deal-...</td>\n",
       "      <td>3 ways to deal with large datasets in Python</td>\n",
       "      <td></td>\n",
       "      <td>1.jpeg</td>\n",
       "      <td>284</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://towardsdatascience.com/from-supervised...</td>\n",
       "      <td>From Supervised To Unsupervised Learning: A Pa...</td>\n",
       "      <td>Slowly removing the…</td>\n",
       "      <td>2.png</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://towardsdatascience.com/top-python-libr...</td>\n",
       "      <td>Top Python Libraries for Visualization: A Star...</td>\n",
       "      <td>[The guide to plotting scatter plots, heat map...</td>\n",
       "      <td></td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://towardsdatascience.com/simple-method-o...</td>\n",
       "      <td>Simple method of targeted TF-IDF topic modelin...</td>\n",
       "      <td>Using a targeted TF-IDF Topic…</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>21</td>\n",
       "      <td>[[[]], [Kenneth Hua], [Towards Data Science], ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://towardsdatascience.com/optimizing-pati...</td>\n",
       "      <td>Optimizing Patient Scheduling</td>\n",
       "      <td>Efficient Clinic Flow and Reduced…</td>\n",
       "      <td>5.png</td>\n",
       "      <td>99999</td>\n",
       "      <td>[[[]], [Gabe Verzino], [Towards Data Science],...</td>\n",
       "      <td>10</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                url  \\\n",
       "0   1  https://towardsdatascience.com/5-ways-to-deal-...   \n",
       "1   2  https://towardsdatascience.com/from-supervised...   \n",
       "2   3  https://towardsdatascience.com/top-python-libr...   \n",
       "3   4  https://towardsdatascience.com/simple-method-o...   \n",
       "4   5  https://towardsdatascience.com/optimizing-pati...   \n",
       "\n",
       "                                               title  \\\n",
       "0       3 ways to deal with large datasets in Python   \n",
       "1  From Supervised To Unsupervised Learning: A Pa...   \n",
       "2  Top Python Libraries for Visualization: A Star...   \n",
       "3  Simple method of targeted TF-IDF topic modelin...   \n",
       "4                      Optimizing Patient Scheduling   \n",
       "\n",
       "                                            subtitle   image  claps  \\\n",
       "0                                                     1.jpeg    284   \n",
       "1                               Slowly removing the…   2.png    261   \n",
       "2  [The guide to plotting scatter plots, heat map...            102   \n",
       "3                     Using a targeted TF-IDF Topic…   4.jpg     21   \n",
       "4                 Efficient Clinic Flow and Reduced…   5.png  99999   \n",
       "\n",
       "                                           responses  reading_time  \\\n",
       "0                                                  2             3   \n",
       "1                                                  1             6   \n",
       "2                                                  1             8   \n",
       "3  [[[]], [Kenneth Hua], [Towards Data Science], ...             5   \n",
       "4  [[[]], [Gabe Verzino], [Towards Data Science],...            10   \n",
       "\n",
       "            publication        date  \n",
       "0  Towards Data Science  2022-01-01  \n",
       "1  Towards Data Science  2022-01-01  \n",
       "2  Towards Data Science  2022-01-01  \n",
       "3  Towards Data Science  2022-01-01  \n",
       "4  Towards Data Science  2022-01-01  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data head\n",
    "medium_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to csv file\n",
    "medium_df.to_csv('data/medium_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
